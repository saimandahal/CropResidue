{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import onnx\n",
    "\n",
    "import tifffile as tiff\n",
    "\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataloader\n",
    "\n",
    "class CropDataloader(Dataset):\n",
    "    \n",
    "    # Dataset => image and mask pairs. Images in RGB.   \n",
    "\n",
    "    def __init__(self, image_directory, mask_directory):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_directory, \"*.jpg\")))\n",
    "        self.mask_paths = sorted(glob.glob(os.path.join(mask_directory, \"*.tif\")))\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "\n",
    "        mask = Image.open(self.mask_paths[index])\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, masks = zip(*batch)\n",
    "\n",
    "    return list(images), list(masks)\n",
    "\n",
    "# Image path\n",
    "\n",
    "# Training dataloader\n",
    "\n",
    "image_path_train = \"/local/data/sdahal_p/Crop/data/train/images/\"\n",
    "mask_image_path_train = \"/local/data/sdahal_p/Crop/data/train/masks/\"\n",
    "\n",
    "dataset_train = CropDataloader(image_path_train, mask_image_path_train)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "# validation dataloader\n",
    "\n",
    "image_path_validation = \"/local/data/sdahal_p/Crop/data/validation/images/\"\n",
    "mask_image_path_validation = \"/local/data/sdahal_p/Crop/data/validation/masks/\"\n",
    "\n",
    "dataset_validation = CropDataloader(image_path_validation, mask_image_path_validation)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_validation, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "\n",
    "class CropDataloaderTest(Dataset):\n",
    "    \n",
    "    def __init__(self, image_directory):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_directory, \"*.jpg\")))\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "\n",
    "        return image\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "\n",
    "    return list(batch)\n",
    "\n",
    "image_path = \"/local/data/sdahal_p/Crop/data/test/\"\n",
    "\n",
    "dataset_test = CropDataloaderTest(image_path)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IoU function\n",
    "\n",
    "def compute_iou(preds, target, num_classes=4):\n",
    "\n",
    "        iou_per_class = []\n",
    "        \n",
    "        for cls in range(num_classes):\n",
    "\n",
    "            pred_cls = preds == cls\n",
    "            \n",
    "            target_cls = target == cls\n",
    "\n",
    "            intersection = torch.logical_and(pred_cls, target_cls).sum().item()\n",
    "            \n",
    "            union = torch.logical_or(pred_cls, target_cls).sum().item()\n",
    "\n",
    "            if union == 0:\n",
    "                \n",
    "                iou_per_class.append(float('nan'))  \n",
    "\n",
    "            else:\n",
    "                \n",
    "                iou_per_class.append(intersection / union)\n",
    "\n",
    "        \n",
    "        \n",
    "        return iou_per_class  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "def func_confusion_matrix(all_preds, all_targets, class_id, class_mapping):  \n",
    "\n",
    "    binary_targets = (np.array(all_targets) == class_id).astype(int)\n",
    "    binary_preds = (np.array(all_preds) == class_id).astype(int)\n",
    "\n",
    "    binary_conf_matrix = confusion_matrix(binary_targets, binary_preds)\n",
    "\n",
    "    binary_conf_matrix_percent = binary_conf_matrix.astype(np.float32) / binary_conf_matrix.sum() * 100\n",
    "\n",
    "    # Class name from mapping\n",
    "    class_name = class_mapping.get(class_id, f\"Class {class_id}\")\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(binary_conf_matrix_percent, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.title(f\"Confusion Matrix for {class_name} (Percentage)\")\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "\n",
    "# Model\n",
    "model_name = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "\n",
    "num_classes = 4  \n",
    "\n",
    "image_processor = SegformerImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# \n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "\n",
    "    model_name,\n",
    "    \n",
    "    num_labels=num_classes,  \n",
    "\n",
    "    ignore_mismatched_sizes=True\n",
    "\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 4 ,gamma = 0.6, last_epoch= -1, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "\n",
    "\n",
    "num_epochs = 32\n",
    "\n",
    "epoch_mean_iou = []\n",
    "\n",
    "training_iou = []\n",
    "\n",
    "validation_iou = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()  \n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    num_batches = 0\n",
    "\n",
    "    total_mean_iou = 0.0\n",
    "\n",
    "    for images, masks in dataloader_train:\n",
    "\n",
    "        encoding = image_processor(\n",
    "\n",
    "            images=images,\n",
    "            segmentation_maps=masks,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        pixel_values = encoding[\"pixel_values\"].to(device)\n",
    "         \n",
    "        labels = encoding[\"labels\"].to(device)             \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    logits = outputs.logits\n",
    "\n",
    "\n",
    "    # saving the model \n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "\n",
    "        checkpoint = {\n",
    "        'epoch': epoch, \n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, '/local/data/sdahal_p/genome-test/model_checkpoint.pth')\n",
    "\n",
    "    logits_upsampled = F.interpolate(logits, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    preds = torch.argmax(logits_upsampled, dim=1)\n",
    "\n",
    "    iou_scores_training = compute_iou(preds, labels)\n",
    "\n",
    "    mean_iou_training = torch.nanmean(torch.tensor(iou_scores_training))  \n",
    "\n",
    "    epoch_mean_iou.append(mean_iou_training)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # print(f\"Training IoU per class: {iou_scores_training}\")\n",
    "    # print(f\"Mean IoU training: {mean_iou_training}\")\n",
    "\n",
    "    mean_iou_training = mean_iou_training\n",
    "\n",
    "    # Validation:\n",
    "\n",
    "    validation_loss = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "\n",
    "    all_targets = []\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for images, masks in dataloader_validation:\n",
    "\n",
    "            encoding = image_processor(\n",
    "\n",
    "                images=images,\n",
    "                segmentation_maps=masks,\n",
    "                return_tensors=\"pt\"\n",
    "\n",
    "            )\n",
    "\n",
    "            pixel_values = encoding[\"pixel_values\"].to(device) \n",
    "\n",
    "            labels = encoding[\"labels\"].to(device)             \n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            predicted_segmentation = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.view(-1).cpu().numpy())  \n",
    "\n",
    "            all_targets.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        logits_upsampled = F.interpolate(logits, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        preds = torch.argmax(logits_upsampled, dim=1)\n",
    "\n",
    "        iou_scores_validation = compute_iou(preds, labels)\n",
    "\n",
    "        mean_iou_validation = torch.nanmean(torch.tensor(iou_scores_validation))  \n",
    "\n",
    "        epoch_mean_iou.append(mean_iou_validation.item())\n",
    "\n",
    "        # print(f\"Validation IoU per class: {iou_scores_validation}\")\n",
    "        # print(f\"Mean IoU Validation: {mean_iou_validation.item()}\")\n",
    "\n",
    "        if epoch == 15:\n",
    "\n",
    "            all_preds.extend(preds.view(-1).cpu().numpy())  \n",
    "\n",
    "            all_targets.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "            all_preds = np.array(all_preds)\n",
    "            \n",
    "            all_targets = np.array(all_targets)\n",
    "\n",
    "            for index in range(4):\n",
    "                class_id = index\n",
    "\n",
    "                class_mapping = {\n",
    "                    0: \"Residue with sunlight\",\n",
    "                    1: \"Residue with shade\",\n",
    "                    2: \"Background with sunlight\",\n",
    "                    3: \"Background with shade\"\n",
    "                }\n",
    "\n",
    "                func_confusion_matrix(all_preds, all_targets, class_id, class_mapping)\n",
    "\n",
    "\n",
    "    avg_val_loss = validation_loss/len(dataloader_validation)\n",
    "    \n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    training_iou.append(mean_iou_training)\n",
    "\n",
    "    validation_iou.append(mean_iou_validation)\n",
    "\n",
    "    \n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "csv_filename = \"/local/data/sdahal_p/Crop/iou_scores_log.csv\"\n",
    "\n",
    "\n",
    "epochs = list(range(1, len(training_iou) + 1))\n",
    "\n",
    "training_iou = [iou.item() for iou in training_iou]\n",
    "validation_iou = [iou.item() for iou in validation_iou]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Epoch\": epochs,\n",
    "    \"Training IoU\": training_iou,\n",
    "    \"Validation IoU\": validation_iou\n",
    "})\n",
    "\n",
    "df.to_csv(csv_filename, mode='w', index=False)\n",
    "\n",
    "# Training ends\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "\n",
    "original_dir = \"/local/data/sdahal_p/Crop/original_images\"\n",
    "predicted_dir = \"/local/data/sdahal_p/Crop/predicted_segmentations\"\n",
    "area_dir = \"/local/data/sdahal_p/Crop/area_reports\"\n",
    "\n",
    "os.makedirs(original_dir, exist_ok=True)\n",
    "os.makedirs(predicted_dir, exist_ok=True)\n",
    "os.makedirs(area_dir, exist_ok=True)\n",
    "\n",
    "class_labels = {0: \"Residue with sunlight\", 1: \"Residue with shade 1\", 2: \"Background with sunlight\", 3: \"Background with shade\"}\n",
    "class_colors = [\"black\", \"red\", \"green\", \"blue\"]  \n",
    "\n",
    "cmap = mcolors.ListedColormap(class_colors)\n",
    "bounds = list(class_labels.keys()) + [len(class_labels)]\n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "pixel_size_mm2 = 0.01  \n",
    "\n",
    "\n",
    "output_folder = \"segmentation_results\"\n",
    "os.makedirs(output_folder, exist_ok=True) \n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, images in enumerate(dataloader_test):\n",
    "\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images_pil = [to_pil(img) for img in images]\n",
    "        else:\n",
    "            images_pil = images  \n",
    "\n",
    "        encoding = image_processor(images=images_pil, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].to(device)\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        logits_upsampled = F.interpolate(logits, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "        predicted_segmentation = torch.argmax(logits_upsampled, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "        if predicted_segmentation.dtype != np.uint8:\n",
    "            \n",
    "            predicted_segmentation = predicted_segmentation.astype(np.uint8)\n",
    "\n",
    "        filename = os.path.join(output_folder, f\"segmentation_{idx}.tif\")\n",
    "\n",
    "        tiff.imwrite(filename, predicted_segmentation)\n",
    "\n",
    "        original_image_path = os.path.join(original_dir, f\"image_{idx}.png\")\n",
    "        images_pil[0].save(original_image_path)\n",
    "\n",
    "        predicted_image_path = os.path.join(predicted_dir, f\"segmentation_{idx}.png\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        img = ax.imshow(predicted_segmentation, cmap=cmap, norm=norm)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        cbar = plt.colorbar(img, ticks=list(class_labels.keys()))\n",
    "        cbar.ax.set_yticklabels(list(class_labels.values()))  \n",
    "\n",
    "        plt.savefig(predicted_image_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        area_report = {}\n",
    "        for class_id, class_name in class_labels.items():\n",
    "            pixel_count = np.sum(predicted_segmentation == class_id)\n",
    "            area_mm2 = pixel_count * pixel_size_mm2\n",
    "            area_report[class_name] = area_mm2\n",
    "\n",
    "        area_report_path = os.path.join(area_dir, f\"area_{idx}.txt\")\n",
    "        with open(area_report_path, \"w\") as f:\n",
    "            for class_name, area in area_report.items():\n",
    "                f.write(f\"{class_name}: {area:.2f} mm²\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
